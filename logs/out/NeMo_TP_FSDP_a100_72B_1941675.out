world size: 64, GBS: 128, BSperDev: 16, grad accumulation:1, sequence length: 4096, grad_check: True,  attention: flash_attention_2, Liger-Kernels: False
[NeMo I 2025-11-24 16:46:57 nemo_logging:393] Using passed HF dataset Dataset({
        features: ['id', 'messages', 'source'],
        num_rows: 939343
    })
[NeMo I 2025-11-24 16:47:00 nemo_logging:393] use_linear_ce_loss: True
Using FSDP2 with DP=8, TP=8, CP=1
[NeMo I 2025-11-24 16:47:00 nemo_logging:393] Using default TP plan for parallelization. It is compatible with huggingface llama3-style models.
[NeMo I 2025-11-24 16:47:00 nemo_logging:393] Experiments will be logged at /lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_experiments/default/2025-11-24_16-47-00
NCCL version 2.27.3+cuda12.9
[NeMo I 2025-11-24 16:47:10 nemo_logging:393] Configuring model with attn_implementation: flash_attention_2
NCCL version 2.27.3+cuda12.9
NCCL version 2.27.3+cuda12.9
NCCL version 2.27.3+cuda12.9
NCCL version 2.27.3+cuda12.9
NCCL version 2.27.3+cuda12.9
NCCL version 2.27.3+cuda12.9
NCCL version 2.27.3+cuda12.9
Pre-loop Model MaxMemory for GPU:0 19.307358264923096 GBytes
NCCL version 2.27.3+cuda12.9
NCCL version 2.27.3+cuda12.9
NCCL version 2.27.3+cuda12.9
NCCL version 2.27.3+cuda12.9
NCCL version 2.27.3+cuda12.9
NCCL version 2.27.3+cuda12.9
NCCL version 2.27.3+cuda12.9
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/7339 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/7339 [00:00<?, ?it/s] Mon Nov 24 05:36:45 PM CET 2025

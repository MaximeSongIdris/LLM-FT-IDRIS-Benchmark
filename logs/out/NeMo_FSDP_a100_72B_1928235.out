world size: 64, GBS: 128, BSperDev: 1, grad accumulation:2, sequence length: 4096, grad_check: True,  attention: flash_attention_2, Liger-Kernels: False
[NeMo I 2025-11-24 07:08:18 nemo_logging:393] Using passed HF dataset Dataset({
        features: ['id', 'messages', 'source'],
        num_rows: 939343
    })
[NeMo I 2025-11-24 07:08:21 nemo_logging:393] use_linear_ce_loss: True
Using FSDP2 with DP=64, TP=1, CP=1
[NeMo I 2025-11-24 07:08:21 nemo_logging:393] Using default TP plan for parallelization. It is compatible with huggingface llama3-style models.
[NeMo I 2025-11-24 07:08:21 nemo_logging:393] Experiments will be logged at /lustre/fswork/projects/idris/sos/ssos040/Bench_InstructFT_Tulu3/InstructFT/nemo_experiments/default/2025-11-24_07-08-21
NCCL version 2.27.3+cuda12.9
[NeMo I 2025-11-24 07:17:55 nemo_logging:393] Configuring model with attn_implementation: flash_attention_2
Mon Nov 24 07:31:11 AM CET 2025
